# ğŸ„ Taller de Aprendizaje AutomÃ¡tico No Supervisado con el Dataset de Setas

Este repositorio contiene un taller prÃ¡ctico orientado al aprendizaje automÃ¡tico **no supervisado**, usando tÃ©cnicas de **PCA** y **Clustering (KMeans)**, junto con una comparativa con un modelo supervisado (Random Forest).

Usaremos el **Mushroom Dataset**, un conjunto de datos muy conocido en el Ã¡mbito educativo que contiene informaciÃ³n sobre diferentes tipos de hongos, incluyendo su clasificaciÃ³n como **comestibles o venenosos**.

---

## ğŸ“‚ Dataset

Puedes obtener el dataset desde el siguiente enlace:

ğŸ”— [Mushroom Dataset - UCI Repository](https://archive.ics.uci.edu/ml/datasets/Mushroom)

- **Instancia:** Cada fila representa un hongo.
- **Variables:** Todas son **categÃ³ricas** (forma, color, olor, etc.).
- **Variable objetivo (`class`)**: Binaria â€” `e` (edible/comestible) o `p` (poisonous/venenoso).

---

## ğŸ§  Objetivos del taller

- Cargar y explorar un dataset categÃ³rico complejo.
- Tratar valores nulos y eliminar columnas no informativas.
- Codificar variables categÃ³ricas usando **One-Hot Encoding**.
- Reducir dimensionalidad con **PCA (AnÃ¡lisis de Componentes Principales)**.
- Aplicar **K-Means Clustering** para detectar estructuras ocultas en los datos.
- Comparar el rendimiento del modelo no supervisado con un modelo supervisado (**Random Forest**).

---

## ğŸ”§ TecnologÃ­as utilizadas

- Python
- Pandas / NumPy
- Seaborn / Matplotlib
- Scikit-learn (`PCA`, `KMeans`, `RandomForestClassifier`)

---

## ğŸ—‚ï¸ Contenido del notebook

### 1. ğŸ“¥ Carga y exploraciÃ³n de datos
- VisualizaciÃ³n general del dataset.
- Conteo de valores nulos y valores Ãºnicos por variable.
- EliminaciÃ³n de columnas constantes o poco informativas.

### 2. ğŸ§¼ Preprocesamiento
- ImputaciÃ³n o eliminaciÃ³n de valores faltantes.
- ConversiÃ³n de variables categÃ³ricas con **OneHotEncoder**.
- SeparaciÃ³n entre `X` (features) e `y` (class).

### 3. ğŸ§ª PCA (AnÃ¡lisis de Componentes Principales)
- ReducciÃ³n de dimensionalidad a 2 componentes.
- VisualizaciÃ³n de los datos en 2D.
- EvaluaciÃ³n visual de separabilidad.

### 4. ğŸŒ³ ClasificaciÃ³n supervisada (Random Forest)
- Entrenamiento de un modelo de clasificaciÃ³n.
- EvaluaciÃ³n con mÃ©tricas de precisiÃ³n.
- Estudio del impacto del nÃºmero de componentes en el rendimiento del modelo.

### 5. ğŸ” Clustering con K-Means
- DeterminaciÃ³n del nÃºmero Ã³ptimo de clusters (mÃ©todo del codo).
- Entrenamiento y visualizaciÃ³n de clusters.
- EvaluaciÃ³n de la correspondencia entre clusters y clases reales (sin usar las etiquetas).

---

## ğŸ“Š Resultados esperados

- ReducciÃ³n de >90% de las dimensiones originales sin perder rendimiento.
- ClasificaciÃ³n de precisiÃ³n alta (>95%) usando Random Forest.
- Descubrimiento de estructuras latentes con KMeans, sin necesidad de etiquetas.

---

## ğŸ“Œ Notas 

- El objetivo de este notebook es **introducir conceptos clave de aprendizaje no supervisado**, y para **comparar tÃ©cnicas supervisadas y no supervisadas en la prÃ¡ctica**.



